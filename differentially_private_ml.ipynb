{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Some useful utilities\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "def z_clip(xs, b):\n",
    "    return [min(x, b) for x in xs]\n",
    "\n",
    "def clip(xs, upper, lower):\n",
    "    return [max(min(x, upper), lower) for x in xs]\n",
    "\n",
    "def gaussian_mech_vec(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon, size=len(v))\n",
    "\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def your_code_here():\n",
    "    return 1\n",
    "\n",
    "def test(msg, value, expected):\n",
    "    if value == expected:\n",
    "        print(f\"{msg}: {value}, as expected\")\n",
    "    else:\n",
    "        print(f\"{msg}: OH NO! Got {value}, but expected {expected}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('adult_processed_x.npy')\n",
    "y = np.load('adult_processed_y.npy')\n",
    "\n",
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(theta, xi):\n",
    "    label = np.sign(xi @ theta)\n",
    "    return label\n",
    "\n",
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))\n",
    "\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(theta, X_test) == y_test)/X_test.shape[0]\n",
    "\n",
    "# Simple gradient descent algorithm\n",
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "def gradient_descent(iterations):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787483414418399"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = gradient_descent(10)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (30 points)\n",
    "\n",
    "Implement a function `dp_gradient_descent` that performs differentially private gradient descent by adding noise to the gradient at each iteration. Your function should take additional arguments $\\epsilon$ and $\\delta$, and should have an **overall** privacy cost of $(\\epsilon, \\delta)$-differential privacy. You may target *either* bounded or unbounded differential privacy.\n",
    "\n",
    "**Note**: this is a major difference from the function defined in the notes, which bounds $\\epsilon$ *per-iteration*. Your solution should bound the *total* privacy cost of training.\n",
    "\n",
    "*Hint*: Use `gaussian_mech_vec`, defined above, to add noise.\n",
    "\n",
    "*Hint*: Use advanced composition to bound the total privacy cost. Start with the total privacy cost of $k$-fold adaptive composition under advanced composition, then solve for $\\epsilon_i$, the privacy cost per iteration. Use this result to set the per-iteration value of `epsilon`, and similar for `delta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = 0.01, final accuracy: 0.7533171163202123\n",
      "Epsilon = 0.1, final accuracy: 0.7749889429455993\n",
      "Epsilon = 1.0, final accuracy: 0.7785272003538257\n"
     ]
    }
   ],
   "source": [
    "# Calculate sum with an enforced L2 sensitivity of clipped X\n",
    "def clipped_gradients_sum(theta, X, y, b):\n",
    "    gradients = [gradient(theta, X_i, y_i) for X_i, y_i in zip(X,y)]\n",
    "    return np.sum(gradients, axis=0)\n",
    "\n",
    "def dp_gradient_descent(iterations, epsilon, delta):\n",
    "\n",
    "    # Constants\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    noisy_count_epsilon = epsilon * 0.25\n",
    "    gradient_descent_epsilon = epsilon * 0.75\n",
    "    epsilon_i = gradient_descent_epsilon / iterations\n",
    "    delta_i = delta/iterations\n",
    "    \n",
    "    # Define a global sensitivity\n",
    "    sensitivity = 5\n",
    "    \n",
    "    # Calculate noisy clipped data\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, noisy_count_epsilon)\n",
    "    clipped_X = [L2_clip(X_i, sensitivity) for X_i in X_train]\n",
    "    \n",
    "    # Perform gradient descent\n",
    "    for i in range(iterations):\n",
    "        gradient_sum = clipped_gradients_sum(theta, clipped_X, y_train, sensitivity)\n",
    "        noisy_gradient_sum = gaussian_mech_vec(gradient_sum, sensitivity, epsilon_i, delta_i)\n",
    "        noisy_avg_gradient = noisy_gradient_sum/noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "        \n",
    "    return theta\n",
    "\n",
    "delta = 1e-5\n",
    "\n",
    "for epsilon in [0.01, 0.1, 1.0]:\n",
    "    theta = dp_gradient_descent(10, epsilon, delta)\n",
    "    acc = accuracy(theta)\n",
    "    print(f\"Epsilon = {epsilon}, final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 points)\n",
    "\n",
    "In 2-5 sentences, argue that your implementation of `dp_gradient_descent` satisfies $(\\epsilon, \\delta)$-differential privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The privacy cost of this algorithm would normally be ($k\\epsilon + \\epsilon, k\\delta$): the gradient descent process satisfies ($\\epsilon, \\delta$)-differential privacy, and we make an additional query in order to determine the noisy count. To reduce this privacy cost, the algortihm uses a quarter of the privacy budget to determine the noisy count, and the larger rest to perform the gradient descent which itself uses a total of $\\epsilon_i$ per iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (40 points)\n",
    "\n",
    "Implement a function `zcdp_gradient_descent` that performs differentially private gradient descent by adding noise to the gradient at each iteration. Your function should take an additional argument $\\rho$, and should have an **overall** privacy cost of $\\rho$-zero concentrated differential privacy. You will also have to implement `gaussian_mech_vec_zcdp`, the vector-valued gaussian mechanism for zCDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho = 1e-05, final accuracy: 0.7585139318885449\n",
      "rho = 0.0001, final accuracy: 0.7639318885448917\n",
      "rho = 0.001, final accuracy: 0.7838345864661654\n"
     ]
    }
   ],
   "source": [
    "def gaussian_mech_zCDP_vec(v, sensitivity, rho):\n",
    "    \n",
    "    # Calculate normal dist scale\n",
    "    sigma = np.sqrt((sensitivity**2) / (2*rho))\n",
    "    return v + np.random.normal(scale=sigma)\n",
    "\n",
    "def zcdp_gradient_descent(iterations, rho):\n",
    "    \n",
    "    # Constants\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    rho_i = rho/iterations\n",
    "    \n",
    "    # Define a global sensitivity\n",
    "    sensitivity = 5\n",
    "    \n",
    "    # Calculate noisy clipped data\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, rho)\n",
    "    clipped_X = [L2_clip(X_i, sensitivity) for X_i in X_train]\n",
    "    \n",
    "    # Perform gradient descent\n",
    "    for i in range(iterations):\n",
    "        gradient_sum = clipped_gradients_sum(theta, clipped_X, y_train, sensitivity)\n",
    "        noisy_gradient_sum = gaussian_mech_zCDP_vec(gradient_sum, sensitivity, rho_i)\n",
    "        noisy_avg_gradient = noisy_gradient_sum/noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "        \n",
    "    return theta\n",
    "\n",
    "for rho in [0.00001, 0.0001, 0.001]:\n",
    "    theta = zcdp_gradient_descent(10, rho)\n",
    "    acc = accuracy(theta)\n",
    "    print(f\"rho = {rho}, final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Implement a function `convert_zCDP_eps_delta` that converts a $\\rho$-zCDP privacy bound to a $(\\epsilon,\\delta)$-differential privacy bound, given a target $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho = 1e-05, epsilon = 0.021469660262893472, final accuracy: 0.24148606811145512\n",
      "rho = 0.0001, epsilon = 0.06796140424415112, final accuracy: 0.794670499778859\n",
      "rho = 0.001, epsilon = 0.21559660262893474, final accuracy: 0.7783060592658115\n"
     ]
    }
   ],
   "source": [
    "# Implement: epsilon = rho + 2sqrt(rho*log(1/delta))\n",
    "def convert_zCDP_eps_delta(rho, delta):\n",
    "    return rho + (2 * np.sqrt(rho * np.log(1 / delta)))\n",
    "\n",
    "delta = 1e-5\n",
    "\n",
    "for rho in [0.00001, 0.0001, 0.001]:\n",
    "    theta = zcdp_gradient_descent(10, rho)\n",
    "    acc = accuracy(theta)\n",
    "    epsilon = convert_zCDP_eps_delta(rho, delta)\n",
    "    print(f\"rho = {rho}, epsilon = {epsilon}, final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
